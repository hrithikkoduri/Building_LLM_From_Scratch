{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sub Word Tokenization\n",
    "Hybrid of Character and Word Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rules**\n",
    "1. Do not split frequently used words into smaller subwords. For eg - \"boy\" should not be split further\n",
    "2. Split rare words into smaller subwords. For eg - \"boys\" should be split into \"boy\" and \"s\"\n",
    "\n",
    "Advantages\n",
    "1. Helps model learn the different words with same root words, for \"token\", \"tokens\", \"tokenization\" are all related to the same concept\n",
    "2. Helps model learn that \"moderniation\" and \"tokenization\" arr different words made up of different root word but same suffix \"ization\" and are used in same syntatic context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPE is a subword tokenization technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.8.0\n"
     ]
    }
   ],
   "source": [
    "import  importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", tiktoken.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 703, 389, 345, 30, 220, 50256, 8410, 345, 588, 8887, 30, 554, 262, 3329, 11, 314, 4144, 6891, 13, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 5372, 11, 314, 1650, 290, 1100, 257, 1492, 13, 220, 50256, 220]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, how are you? <|endoftext|> DO you like tea? \"\n",
    "    \"In the morning, I drink coffee. <|endoftext|> \"\n",
    "    \"In the sunlit terraces of someunknownplace, I sit and read a book. <|endoftext|> \"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how are you? <|endoftext|> DO you like tea? In the morning, I drink coffee. <|endoftext|> In the sunlit terraces of someunknownplace, I sit and read a book. <|endoftext|> \n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
